{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prices</th>\n",
       "      <th>address</th>\n",
       "      <th>bed_bath_sqft</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>504000.0</td>\n",
       "      <td>2212 Billy Mills Ln Austin TX 78748</td>\n",
       "      <td>3|2|1458</td>\n",
       "      <td>2212 Billy Mills Ln Austin TX 78748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>273900.0</td>\n",
       "      <td>909 Reinli St APT 106 Austin TX 78751</td>\n",
       "      <td>2|2|876</td>\n",
       "      <td>909 Reinli St APT 106 Austin TX 78751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>379900.0</td>\n",
       "      <td>6104 Diamondleaf Bnd Austin TX 78724</td>\n",
       "      <td>3|3|1795</td>\n",
       "      <td>6104 Diamondleaf Bnd Austin TX 78724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>314900.0</td>\n",
       "      <td>12408 La Guardia Ln Del Valle TX 78617</td>\n",
       "      <td>4|2|1448</td>\n",
       "      <td>12408 La Guardia Ln Del Valle TX 78617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>414900.0</td>\n",
       "      <td>14444 Robert I Walker Blvd Austin TX 78728</td>\n",
       "      <td>3|3|1175</td>\n",
       "      <td>14444 Robert I Walker Blvd Austin TX 78728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prices                                       address bed_bath_sqft                                       links\n",
       "0  504000.0          2212 Billy Mills Ln Austin TX 78748      3|2|1458          2212 Billy Mills Ln Austin TX 78748\n",
       "1  273900.0        909 Reinli St APT 106 Austin TX 78751       2|2|876        909 Reinli St APT 106 Austin TX 78751\n",
       "2  379900.0         6104 Diamondleaf Bnd Austin TX 78724      3|3|1795         6104 Diamondleaf Bnd Austin TX 78724\n",
       "3  314900.0       12408 La Guardia Ln Del Valle TX 78617      4|2|1448       12408 La Guardia Ln Del Valle TX 78617\n",
       "4  414900.0   14444 Robert I Walker Blvd Austin TX 78728      3|3|1175   14444 Robert I Walker Blvd Austin TX 78728"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column datatypes are:\n",
      "prices           float64\n",
      "address           object\n",
      "bed_bath_sqft     object\n",
      "links             object\n",
      "dtype: object\n",
      "The dataframe shape is: (90, 4)\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '2212 Billy Mills Ln Austin TX 78748': No schema supplied. Perhaps you meant http://2212 Billy Mills Ln Austin TX 78748?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-59052abf225d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0mzillow_zestimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'links'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreq_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mhome_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h4:contains(\"Home value\")'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         )\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         p.prepare(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL '2212 Billy Mills Ln Austin TX 78748': No schema supplied. Perhaps you meant http://2212 Billy Mills Ln Austin TX 78748?"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import requests\n",
    "import lxml\n",
    "from lxml.html.soupparser import fromstring\n",
    "import prettify\n",
    "import numbers\n",
    "import htmltext\n",
    "\n",
    "#set some display settings for notebooks\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "#add headers in case you use chromedriver (captchas are no fun); namely used for chromedriver\n",
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "#create url variables for each zillow page\n",
    "with requests.Session() as s:\n",
    "    city = 'austin/' #*****change this city to what you want!!!!*****\n",
    "    \n",
    "    url = 'https://www.zillow.com/homes/for_sale/'+city\n",
    "    url2 = 'https://www.zillow.com/homes/for_sale/'+city+'/2_p/'\n",
    "    url3 = 'https://www.zillow.com/homes/for_sale/'+city+'/3_p/'\n",
    "    url4 = 'https://www.zillow.com/homes/for_sale/'+city+'/4_p/'\n",
    "    url5 = 'https://www.zillow.com/homes/for_sale/'+city+'/5_p/'\n",
    "    url6 = 'https://www.zillow.com/homes/for_sale/'+city+'/6_p/'\n",
    "    url7 = 'https://www.zillow.com/homes/for_sale/'+city+'/7_p/'\n",
    "    url8 = 'https://www.zillow.com/homes/for_sale/'+city+'/8_p/'\n",
    "    url9 = 'https://www.zillow.com/homes/for_sale/'+city+'/9_p/'\n",
    "    url10 = 'https://www.zillow.com/homes/for_sale/'+city+'/10_p/'\n",
    "\n",
    "    r = s.get(url, headers=req_headers)\n",
    "    r2 = s.get(url2, headers=req_headers)\n",
    "    r3 = s.get(url3, headers=req_headers)\n",
    "    r4 = s.get(url4, headers=req_headers)\n",
    "    r5 = s.get(url5, headers=req_headers)\n",
    "    r6 = s.get(url6, headers=req_headers)\n",
    "    r7 = s.get(url7, headers=req_headers)\n",
    "    r8 = s.get(url8, headers=req_headers)\n",
    "    r9 = s.get(url9, headers=req_headers)\n",
    "    r10 = s.get(url10, headers=req_headers)\n",
    "    \n",
    "    url_links = [url, url2, url3, url4, url5, url6, url7, url8, url9, url10]\n",
    "\n",
    "#add contents of urls to soup variable from each url\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "soup1 = BeautifulSoup(r2.content, 'html.parser')\n",
    "soup2 = BeautifulSoup(r3.content, 'html.parser')\n",
    "soup3 = BeautifulSoup(r4.content, 'html.parser')\n",
    "soup4 = BeautifulSoup(r5.content, 'html.parser')\n",
    "soup5 = BeautifulSoup(r6.content, 'html.parser')\n",
    "soup6 = BeautifulSoup(r7.content, 'html.parser')\n",
    "soup7 = BeautifulSoup(r8.content, 'html.parser')\n",
    "soup8 = BeautifulSoup(r9.content, 'html.parser')\n",
    "soup9 = BeautifulSoup(r10.content, 'html.parser')\n",
    "\n",
    "# page_links = [soup, soup1, soup2, soup3, soup4, soup5, soup6, soup7, soup8, soup9]\n",
    "\n",
    "#create the first two dataframes\n",
    "df = pd.DataFrame()\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\n",
    "for i in soup:\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df['prices'] = price\n",
    "    df['address'] = address\n",
    "    df['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "#     print(href)\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "#         addresses = 'zillow doesnt like this piece of code!'\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "#     print(addresses)\n",
    "#     addresses = addresses.get_text()\n",
    "#     addresses = href.find_all(href=True)\n",
    "#     addresses = href.find('address class')\n",
    "#     addresses.extract()\n",
    "#     print(addresses)\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "df['links'] = urls\n",
    "df['links'] = df['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df['links'] = df['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df['links'] = df['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup1:\n",
    "    address1 = soup1.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup1.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup1.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup1.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup1.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup1.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup1.find_all (class_= 'list-card-link')\n",
    "#     zestimate1 = soup1.find_all (class_= 'Text-c11n-8-38-0__aiai24-0 jtMauM')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df1['prices'] = price1\n",
    "    df1['address'] = address1\n",
    "    df1['beds'] = beds\n",
    "#     df1['zestimate'] = zestimate1\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup1.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df1['links'] = urls\n",
    "df1['links'] = df1['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df1['links'] = df1['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df1['links'] = df1['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "#append first two dataframes\n",
    "df = df.append(df1, ignore_index = True) \n",
    "\n",
    "#create empty dataframes\n",
    "df2 = pd.DataFrame()\n",
    "df3 = pd.DataFrame()\n",
    "df4 = pd.DataFrame()\n",
    "df5 = pd.DataFrame()\n",
    "df6 = pd.DataFrame()\n",
    "df7 = pd.DataFrame()\n",
    "df8 = pd.DataFrame()\n",
    "df9 = pd.DataFrame()\n",
    "\n",
    "for i in soup2:\n",
    "    soup = soup2\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df2['prices'] = price\n",
    "    df2['address'] = address\n",
    "    df2['beds'] = beds\n",
    "\n",
    "time.sleep(3.2)    \n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup2.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df2['links'] = urls\n",
    "df2['links'] = df2['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df2['links'] = df2['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df2['links'] = df2['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "#append n df\n",
    "# df = df.append(df2, ignore_index = True) \n",
    "# display(df)\n",
    "# time.sleep(3.1)\n",
    "\n",
    "for i in soup3:\n",
    "    soup = soup3\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df3['prices'] = price1\n",
    "    df3['address'] = address1\n",
    "    df3['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup3.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df3['links'] = urls\n",
    "df3['links'] = df3['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df3['links'] = df3['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df3['links'] = df3['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup4:\n",
    "    soup = soup4\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df4['prices'] = price1\n",
    "    df4['address'] = address1\n",
    "    df4['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup4.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df4['links'] = urls\n",
    "df4['links'] = df4['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df4['links'] = df4['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df4['links'] = df4['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup5:\n",
    "    soup = soup5\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df5['prices'] = price1\n",
    "    df5['address'] = address1\n",
    "    df5['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup5.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df5['links'] = urls\n",
    "df5['links'] = df5['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df5['links'] = df5['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df5['links'] = df5['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup6:\n",
    "    soup = soup6\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df6['prices'] = price1\n",
    "    df6['address'] = address1\n",
    "    df6['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup6.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "#import urls into a links column\n",
    "df6['links'] = urls\n",
    "df6['links'] = df6['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df6['links'] = df6['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df6['links'] = df6['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup7:\n",
    "    soup = soup7\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df7['prices'] = price1\n",
    "    df7['address'] = address1\n",
    "    df7['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup7.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df7['links'] = urls\n",
    "df7['links'] = df7['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df7['links'] = df7['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df7['links'] = df7['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup8:\n",
    "    soup = soup8\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df8['prices'] = price1\n",
    "    df8['address'] = address1\n",
    "    df8['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup8.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df8['links'] = urls\n",
    "df8['links'] = df8['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df8['links'] = df8['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df8['links'] = df8['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup9:\n",
    "    soup = soup9\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df9['prices'] = price1\n",
    "    df9['address'] = address1\n",
    "    df9['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup9.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df9['links'] = urls\n",
    "df9['links'] = df9['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df9['links'] = df9['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df9['links'] = df9['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "df = df.append(df2, ignore_index = True) \n",
    "df = df.append(df3, ignore_index = True) \n",
    "df = df.append(df4, ignore_index = True) \n",
    "df = df.append(df5, ignore_index = True) \n",
    "df = df.append(df6, ignore_index = True) \n",
    "df = df.append(df7, ignore_index = True) \n",
    "df = df.append(df8, ignore_index = True) \n",
    "df = df.append(df9, ignore_index = True)\n",
    "\n",
    "#convert columns to str\n",
    "df['prices'] = df['prices'].astype('str')\n",
    "df['address'] = df['address'].astype('str')\n",
    "df['beds'] = df['beds'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df['prices'] = df['prices'].replace('<div class=\"list-card-price\">', ' ', regex=True)\n",
    "df['address'] = df['address'].replace('<address class=\"list-card-addr\">', ' ', regex=True)\n",
    "df['prices'] = df['prices'].replace('</div>', ' ', regex=True)\n",
    "df['address'] = df['address'].replace('</address>', ' ', regex=True)\n",
    "df['prices'] = df['prices'].str.replace(r'\\D', '')\n",
    "\n",
    "#remove html tags from beds column \n",
    "df['beds'] = df['beds'].replace('<ul class=\"list-card-details\"><li class=\"\">', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<ul class=\"list-card-details\"><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bds</', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bd</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li></ul>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace(' abbr></li><li class=\"\">', '|', regex=True)\n",
    "df['beds'] = df['beds'].replace('Studio</li><li>', '0 ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->ba</', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace(' abbr></li><li class=\"\">', '|', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li><li class=\"list-card-statusText\">- House for sale</li></ul>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li><li class=\"list-card-statusText\">- Condo for sale</li></ul>', ' ', regex=True)\n",
    "\n",
    "df.columns = ['prices','address','bed_bath_sqft','links']\n",
    "#split beds column into beds, bath and sq_feet\n",
    "# df[['beds','baths','sq_feet']] = df.beds.str.split(expand=True)\n",
    "# df[['baths']] = df.beds.str.split('|',expand=True)\n",
    "# df[['sq_feet']] = df.baths.str.split('|',expand=True)\n",
    "\n",
    "#remove commas from sq_feet and convert to float\n",
    "df.replace(',','', regex=True, inplace=True)\n",
    "\n",
    "#drop nulls\n",
    "df = df[(df['prices'] != '') & (df['prices']!= ' ')]\n",
    "\n",
    "#convert column to float\n",
    "df['prices'] = df['prices'].astype('float')\n",
    "# d['sq_feet'] = df['sq_feet'].astype('float')\n",
    "\n",
    "#remove spaces from link column\n",
    "# df['links'] = df.links.str.replace(' ','')\n",
    "\n",
    "#display\n",
    "display(df.head())\n",
    "\n",
    "print('The column datatypes are:')\n",
    "print(df.dtypes)\n",
    "print('The dataframe shape is:', df.shape)\n",
    "\n",
    "#rearrange the columns\n",
    "# df = df[['prices', 'address', 'links', 'beds', 'baths', 'sq_feet']]\n",
    "\n",
    "# df\n",
    "\n",
    "#calculate the zestimate and insert into a dataframe\n",
    "zillow_zestimate = []\n",
    "for link in df['links']:\n",
    "    r = s.get(link, headers=req_headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    home_value = soup.select_one('h4:contains(\"Home value\")')\n",
    "    if not home_value:\n",
    "        home_value = soup.select_one('.zestimate').text.split()[-1]\n",
    "    else:\n",
    "        home_value = home_value.find_next('p').get_text(strip=True)\n",
    "    zillow_zestimate.append(home_value)\n",
    "\n",
    "cols=['zestimate']\n",
    "zestimate_result = pd.DataFrame(zillow_zestimate, columns=cols)\n",
    "# zestimate_result\n",
    "\n",
    "#convert zestimate column to float, and remove , and $\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace('$','')\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace('/mo','')\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace(',','')\n",
    "\n",
    "#covert rows with non zestimate to 0\n",
    "def non_zestimate(zestimate_result):\n",
    "    if len(zestimate_result['zestimate']) > 20:\n",
    "        return '0'\n",
    "    elif len(zestimate_result['zestimate']) < 5:\n",
    "        return '0'\n",
    "    else:\n",
    "        return zestimate_result['zestimate']\n",
    "\n",
    "zestimate_result['zestimate'] = zestimate_result.apply(non_zestimate,axis=1)\n",
    "\n",
    "# zestimate_result\n",
    "\n",
    "#concat zestimate dataframe and original df\n",
    "df = pd.concat([df, zestimate_result], axis=1)\n",
    "df['zestimate'] = df['zestimate'].astype('float')\n",
    "\n",
    "#create best deal column and sort by best_deal\n",
    "df ['best_deal'] = df['prices'] - df['zestimate']\n",
    "df = df.sort_values(by='best_deal')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-6ff30ee23d48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'address'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddress1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beds'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zestimate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzestimate1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m#create empty url list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3040\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3114\u001b[0m         \"\"\"\n\u001b[1;32m   3115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3764\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3766\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \"\"\"\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    748\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (0) does not match length of index (9)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import requests\n",
    "import lxml\n",
    "from lxml.html.soupparser import fromstring\n",
    "import prettify\n",
    "import numbers\n",
    "import htmltext\n",
    "\n",
    "#set some display settings for notebooks\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "#add headers in case you use chromedriver (captchas are no fun); namely used for chromedriver\n",
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "#create url variables for each zillow page\n",
    "with requests.Session() as s:\n",
    "    city = 'austin/' #*****change this city to what you want!!!!*****\n",
    "    \n",
    "    url = 'https://www.zillow.com/homes/for_sale/'+city\n",
    "    url2 = 'https://www.zillow.com/homes/for_sale/'+city+'/2_p/'\n",
    "    url3 = 'https://www.zillow.com/homes/for_sale/'+city+'/3_p/'\n",
    "    url4 = 'https://www.zillow.com/homes/for_sale/'+city+'/4_p/'\n",
    "    url5 = 'https://www.zillow.com/homes/for_sale/'+city+'/5_p/'\n",
    "    url6 = 'https://www.zillow.com/homes/for_sale/'+city+'/6_p/'\n",
    "    url7 = 'https://www.zillow.com/homes/for_sale/'+city+'/7_p/'\n",
    "    url8 = 'https://www.zillow.com/homes/for_sale/'+city+'/8_p/'\n",
    "    url9 = 'https://www.zillow.com/homes/for_sale/'+city+'/9_p/'\n",
    "    url10 = 'https://www.zillow.com/homes/for_sale/'+city+'/10_p/'\n",
    "\n",
    "    r = s.get(url, headers=req_headers)\n",
    "    r2 = s.get(url2, headers=req_headers)\n",
    "    r3 = s.get(url3, headers=req_headers)\n",
    "    r4 = s.get(url4, headers=req_headers)\n",
    "    r5 = s.get(url5, headers=req_headers)\n",
    "    r6 = s.get(url6, headers=req_headers)\n",
    "    r7 = s.get(url7, headers=req_headers)\n",
    "    r8 = s.get(url8, headers=req_headers)\n",
    "    r9 = s.get(url9, headers=req_headers)\n",
    "    r10 = s.get(url10, headers=req_headers)\n",
    "    \n",
    "    url_links = [url, url2, url3, url4, url5, url6, url7, url8, url9, url10]\n",
    "\n",
    "#add contents of urls to soup variable from each url\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "soup1 = BeautifulSoup(r2.content, 'html.parser')\n",
    "soup2 = BeautifulSoup(r3.content, 'html.parser')\n",
    "soup3 = BeautifulSoup(r4.content, 'html.parser')\n",
    "soup4 = BeautifulSoup(r5.content, 'html.parser')\n",
    "soup5 = BeautifulSoup(r6.content, 'html.parser')\n",
    "soup6 = BeautifulSoup(r7.content, 'html.parser')\n",
    "soup7 = BeautifulSoup(r8.content, 'html.parser')\n",
    "soup8 = BeautifulSoup(r9.content, 'html.parser')\n",
    "soup9 = BeautifulSoup(r10.content, 'html.parser')\n",
    "\n",
    "# page_links = [soup, soup1, soup2, soup3, soup4, soup5, soup6, soup7, soup8, soup9]\n",
    "\n",
    "#create the first two dataframes\n",
    "df = pd.DataFrame()\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\n",
    "for i in soup:\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df['prices'] = price\n",
    "    df['address'] = address\n",
    "    df['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "#     print(href)\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "#         addresses = 'zillow doesnt like this piece of code!'\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "#     print(addresses)\n",
    "#     addresses = addresses.get_text()\n",
    "#     addresses = href.find_all(href=True)\n",
    "#     addresses = href.find('address class')\n",
    "#     addresses.extract()\n",
    "#     print(addresses)\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "df['links'] = urls\n",
    "df['links'] = df['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df['links'] = df['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df['links'] = df['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup1:\n",
    "    address1 = soup1.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup1.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup1.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup1.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup1.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup1.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup1.find_all (class_= 'list-card-link')\n",
    "    zestimate1 = soup1.find_all (class_= 'Text-c11n-8-38-0__aiai24-0 jtMauM')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df1['prices'] = price1\n",
    "    df1['address'] = address1\n",
    "    df1['beds'] = beds\n",
    "    df1['zestimate'] = zestimate1\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df1['links'] = urls\n",
    "df1['links'] = df1['links'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all ('div', {'id': 'ds-home-values'}):\n",
    "    print(link)\n",
    "#     href = link.find('a',class_=\"list-card-link\")\n",
    "#     if href == None:\n",
    "#         addresses = None\n",
    "#     else:\n",
    "#         addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "#     urls.append(addresses)\n",
    "#     urls = [x for x in urls if x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
